{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For performing Exploratory Data Analysis (EDA) for a regression task in Python, you typically follow these steps to understand the relationships, distributions, and patterns within the data. Below is a list of some basic EDA techniques using Python for regression problems:\n",
    "\n",
    "### 1. **Loading and Inspecting Data**\n",
    "   - **Read dataset:** `pd.read_csv()` (or other relevant function depending on the data format).\n",
    "   - **Check shape of the dataset:** `df.shape`\n",
    "   - **Display first few rows:** `df.head()`\n",
    "   - **Check data types:** `df.info()`\n",
    "   - **Check for missing values:** `df.isnull().sum()`\n",
    "   - **Summary statistics:** `df.describe()`\n",
    "\n",
    "### 2. **Data Preprocessing**\n",
    "   - **Handling missing values:** `df.fillna()` or `df.dropna()`\n",
    "   - **Handling outliers:** Use visualization or statistical methods (e.g., z-score, IQR).\n",
    "   - **Encoding categorical variables:** `pd.get_dummies()` for one-hot encoding.\n",
    "   - **Feature scaling:** Standardize or normalize numerical features using `StandardScaler` or `MinMaxScaler`.\n",
    "\n",
    "### 3. **Univariate Analysis (Single Variable)**\n",
    "   - **Histogram for numerical features:** \n",
    "     ```python\n",
    "     df['column_name'].hist(bins=30)\n",
    "     ```\n",
    "   - **Boxplot to visualize distribution and outliers:**\n",
    "     ```python\n",
    "     sns.boxplot(x=df['column_name'])\n",
    "     ```\n",
    "   - **Count plot for categorical variables:**\n",
    "     ```python\n",
    "     sns.countplot(x='categorical_column', data=df)\n",
    "     ```\n",
    "   - **Descriptive statistics for numerical variables:**\n",
    "     ```python\n",
    "     df['column_name'].describe()\n",
    "     ```\n",
    "\n",
    "### 4. **Bivariate Analysis (Two Variables)**\n",
    "   - **Scatter plot to visualize relationship between target and features:**\n",
    "     ```python\n",
    "     sns.scatterplot(x='feature_column', y='target_column', data=df)\n",
    "     ```\n",
    "   - **Correlation matrix for numerical features:**\n",
    "     ```python\n",
    "     correlation_matrix = df.corr()\n",
    "     sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "     ```\n",
    "   - **Pair plot to see pairwise relationships:**\n",
    "     ```python\n",
    "     sns.pairplot(df)\n",
    "     ```\n",
    "   - **Box plot between categorical features and target variable (if applicable):**\n",
    "     ```python\n",
    "     sns.boxplot(x='categorical_feature', y='target_column', data=df)\n",
    "     ```\n",
    "\n",
    "### 5. **Multivariate Analysis (Multiple Variables)**\n",
    "   - **3D scatter plot (if you have 3 continuous features):**\n",
    "     ```python\n",
    "     from mpl_toolkits.mplot3d import Axes3D\n",
    "     fig = plt.figure()\n",
    "     ax = fig.add_subplot(111, projection='3d')\n",
    "     ax.scatter(df['feature1'], df['feature2'], df['target'])\n",
    "     ```\n",
    "   - **Heatmap of the correlation matrix** (as shown in Bivariate Analysis but for a larger set of variables).\n",
    "   - **Principal Component Analysis (PCA) to reduce dimensionality and visualize the data:**\n",
    "     ```python\n",
    "     from sklearn.decomposition import PCA\n",
    "     pca = PCA(n_components=2)\n",
    "     principal_components = pca.fit_transform(df[['feature1', 'feature2', 'feature3']])\n",
    "     pca_df = pd.DataFrame(data=principal_components, columns=['PCA1', 'PCA2'])\n",
    "     sns.scatterplot(x='PCA1', y='PCA2', data=pca_df)\n",
    "     ```\n",
    "\n",
    "### 6. **Visualizing Relationships and Checking Assumptions**\n",
    "   - **Linear regression plot (to visualize linearity assumption):**\n",
    "     ```python\n",
    "     sns.regplot(x='feature_column', y='target_column', data=df)\n",
    "     ```\n",
    "   - **Residual plot (check homoscedasticity assumption):**\n",
    "     ```python\n",
    "     sns.residplot(x='feature_column', y='target_column', data=df)\n",
    "     ```\n",
    "   - **Plotting residuals:**\n",
    "     ```python\n",
    "     residuals = y - model.predict(X)\n",
    "     sns.histplot(residuals, kde=True)\n",
    "     ```\n",
    "\n",
    "### 7. **Multicollinearity Check**\n",
    "   - **Variance Inflation Factor (VIF):**\n",
    "     ```python\n",
    "     from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "     vif_data = pd.DataFrame()\n",
    "     vif_data[\"Feature\"] = df.columns\n",
    "     vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(len(df.columns))]\n",
    "     print(vif_data)\n",
    "     ```\n",
    "\n",
    "### 8. **Checking Normality**\n",
    "   - **Normality of the target variable:**\n",
    "     ```python\n",
    "     sns.histplot(df['target_column'], kde=True)\n",
    "     ```\n",
    "   - **Q-Q plot (Quantile-Quantile plot) for normality:**\n",
    "     ```python\n",
    "     import scipy.stats as stats\n",
    "     stats.probplot(df['target_column'], dist=\"norm\", plot=plt)\n",
    "     ```\n",
    "\n",
    "### 9. **Handling Categorical Data (if applicable)**\n",
    "   - **Bar plot for categorical vs target:**\n",
    "     ```python\n",
    "     sns.barplot(x='categorical_feature', y='target_column', data=df)\n",
    "     ```\n",
    "   - **One-hot encoding:**\n",
    "     ```python\n",
    "     df = pd.get_dummies(df, columns=['categorical_feature'])\n",
    "     ```\n",
    "\n",
    "---\n",
    "\n",
    "These techniques will give you a good understanding of your dataset and its suitability for regression modeling. After performing these EDA steps, you can make informed decisions about preprocessing, feature engineering, and which model to apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reg Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Mean Absolute Error (MAE)**  \n",
    "   - Measures the average of absolute differences between predicted and actual values.  \n",
    "   - Useful when you want a simple metric and care equally about all errors.  \n",
    "   - Lower MAE indicates better predictive accuracy.\n",
    "\n",
    "2. **Mean Squared Error (MSE)**  \n",
    "   - Measures the average of squared differences between predicted and actual values.  \n",
    "   - Preferred when larger errors are particularly undesirable.  \n",
    "   - Lower MSE signifies better performance, with more penalty for large errors.\n",
    "\n",
    "3. **Root Mean Squared Error (RMSE)**  \n",
    "   - Square root of MSE, providing error in the original unit of measurement.  \n",
    "   - Useful when you want to interpret the error in the same units as the data.  \n",
    "   - Lower RMSE means better model performance, with an emphasis on large errors.\n",
    "\n",
    "4. **R-squared (R²)**  \n",
    "   - Represents the proportion of variance in the dependent variable explained by the model.  \n",
    "   - Good for assessing overall model fit and how well the model captures variability.  \n",
    "   - Values closer to 1 indicate better fit; 0 means no explanatory power.\n",
    "\n",
    "5. **Adjusted R-squared**  \n",
    "   - R² adjusted for the number of predictors, penalizing excessive variables.  \n",
    "   - Use when comparing models with different numbers of predictors.  \n",
    "   - Higher values indicate a better fit, adjusted for model complexity.\n",
    "\n",
    "6. **Mean Absolute Percentage Error (MAPE)**  \n",
    "   - Measures the average percentage difference between predicted and actual values.  \n",
    "   - Useful when you need an error metric that is independent of scale and easily interpretable.  \n",
    "   - Lower MAPE indicates better model performance, with less bias toward large/small values.\n",
    "\n",
    "7. **Explained Variance Score**  \n",
    "   - Indicates the proportion of the variance explained by the model’s predictions.  \n",
    "   - Good for evaluating how much the model explains the variation in the data.  \n",
    "   - Values closer to 1 show better prediction and variance capture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Goal\n",
    "You want to reshape a **1D array** (like a Pandas Series or a 1D NumPy array) into a **2D array** with one column. The reason you're using `-1, 1` is to tell NumPy how to reshape the array in a flexible but specific way.\n",
    "\n",
    "### The 1D Array\n",
    "Suppose you have a 1D array, such as:\n",
    "```python\n",
    "X_train = [1, 2, 3, 4, 5]\n",
    "```\n",
    "\n",
    "This is a 1D array with 5 elements. Its shape is `(5,)`, meaning it has 5 values but only one row.\n",
    "\n",
    "### Why `reshape(-1, 1)`?\n",
    "\n",
    "When you use `reshape(-1, 1)`, you are telling NumPy the following:\n",
    "\n",
    "- **`-1`**: \"I don't want to specify how many rows I want. Let NumPy figure that out for me based on the length of the array.\"\n",
    "- **`1`**: \"I want **1 column**. Each value in my original array should be placed into its own row, but all in one column.\"\n",
    "\n",
    "#### What Happens?\n",
    "Let's break it down with an example:\n",
    "\n",
    "1. **Before reshaping**: \n",
    "   - Original array: `[1, 2, 3, 4, 5]`\n",
    "   - Shape: `(5,)` (5 elements in a single row)\n",
    "\n",
    "2. **Reshaped with `.reshape(-1, 1)`**:\n",
    "   ```python\n",
    "   X_train = X_train.reshape(-1, 1)\n",
    "   ```\n",
    "   - New array:\n",
    "     ```\n",
    "     [[1],\n",
    "      [2],\n",
    "      [3],\n",
    "      [4],\n",
    "      [5]]\n",
    "     ```\n",
    "   - Shape: `(5, 1)` (5 rows, 1 column)\n",
    "\n",
    "So, with `reshape(-1, 1)`, you **convert the 1D array into a 2D array** with each value in its own row, but there is only **1 column**.\n",
    "\n",
    "### Why Use `-1, 1`?\n",
    "#### **1. Flexibility with Row Number (`-1`)**\n",
    "The `-1` means \"let NumPy decide the number of rows based on the original array's size.\" If your array has 10 elements, `reshape(-1, 1)` will give you a shape of `(10, 1)`. If it has 100 elements, it will reshape it to `(100, 1)`.\n",
    "\n",
    "Without `-1`, you would have to manually count how many rows you need and specify it. `-1` makes this automatic.\n",
    "\n",
    "#### **2. Standard Format for Machine Learning**\n",
    "Many machine learning algorithms, especially in libraries like **scikit-learn**, expect the data in a **2D array format**:\n",
    "\n",
    "- **Rows**: Each row represents a **data point** (one sample).\n",
    "- **Columns**: Each column represents a **feature** (one attribute of the sample).\n",
    "\n",
    "For example, if you have 5 data points (samples) and each data point has 1 feature, you need a **5 x 1 array** (5 rows, 1 column).\n",
    "\n",
    "### Summary:\n",
    "- **`reshape(-1, 1)`** means \"I want to reshape this array into a 2D array with as many rows as needed and 1 column.\"\n",
    "- It’s a flexible way to ensure that the data is in the correct format for algorithms that expect 2D input.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
